{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed87b84-632f-472d-9f93-e9a8b4c95cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic chunk method since tokenizer model(miniLM) wasnt working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3b16c-92aa-483b-bd80-3447f248f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers faiss-cpu rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68b9562-9349-44d2-afee-a30b4b773659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "#path definition\n",
    "ROOT = Path.cwd()\n",
    "CHUNKS_PATH = ROOT / \"artifacts/chunks/chunks.jsonl\"\n",
    "INDEX_DIR   = ROOT / \"artifacts/index\"\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#loading chunk data\n",
    "texts, meta = [], []\n",
    "with open(CHUNKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        r = json.loads(line)\n",
    "        texts.append(r[\"text\"])\n",
    "        meta.append({\"id\": r[\"id\"], \"source_file\": r[\"source_file\"], \"source_url\": r.get(\"source_url\",\"(unknown)\")})\n",
    "\n",
    "len(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90302251-bc8f-4c55-9a38-dce0172c6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e66644e-69f2-4783-a57f-38bd494f5643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324a3f0cdf7844b997c8d448779d673d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, faiss, json\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"  \n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "emb = model.encode(\n",
    "    texts, batch_size=64, convert_to_numpy=True,\n",
    "    show_progress_bar=True, normalize_embeddings=True\n",
    ")\n",
    "index = faiss.IndexFlatIP(emb.shape[1]) \n",
    "index.add(emb)\n",
    "\n",
    "faiss.write_index(index, str(INDEX_DIR / \"faiss.index\"))\n",
    "(INDEX_DIR / \"meta.json\").write_text(json.dumps(meta), encoding=\"utf-8\")\n",
    "(INDEX_DIR / \"model.txt\").write_text(MODEL_NAME, encoding=\"utf-8\")\n",
    "\n",
    "index.ntotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5511c3-64ed-4e3a-a022-95a3c72d5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding bm25 ranker for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b6116f2-301c-4b87-867c-0b23a3cb5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re, numpy as np\n",
    "\n",
    "\n",
    "tokenized = [re.findall(r\"\\w+\", t.lower()) for t in texts]\n",
    "bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "def bm25_search(q, k=50):\n",
    "    scores = bm25.get_scores(re.findall(r\"\\w+\", q.lower()))\n",
    "    idx = np.argsort(scores)[::-1][:k]\n",
    "    return [{\"idx\": int(i), \"bm25\": float(scores[i])} for i in idx]\n",
    "\n",
    "def dense_search(q, k=50):\n",
    "    qv = model.encode([q], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    D, I = index.search(qv, k)\n",
    "    return [{\"idx\": int(i), \"dense\": float(s)} for i, s in zip(I[0], D[0])]\n",
    "\n",
    "def rrf_fuse(q, k=10, K=60):\n",
    "    a = bm25_search(q, k=50)\n",
    "    b = dense_search(q, k=50)\n",
    "    rank = {}\n",
    "    for results in (a, b):\n",
    "        for pos, item in enumerate(results, start=1):\n",
    "            rank[item[\"idx\"]] = rank.get(item[\"idx\"], 0.0) + 1.0/(K + pos)\n",
    "    order = sorted(rank.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    fused = []\n",
    "    for i, score in order:\n",
    "        fused.append({\n",
    "            \"score_rrf\": float(score),\n",
    "            \"source_url\": meta[i][\"source_url\"],\n",
    "            \"source_file\": meta[i][\"source_file\"],\n",
    "            \"text\": texts[i]\n",
    "        })\n",
    "    return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9a0bb3-e390-4d5c-920c-cb43fe6ac57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder(\"BAAI/bge-reranker-base\")  \n",
    "\n",
    "def rerank(query, candidates, topk=5):\n",
    "    pairs = [(query, c[\"text\"]) for c in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    for c, s in zip(candidates, scores):\n",
    "        c[\"rerank\"] = float(s)\n",
    "    return sorted(candidates, key=lambda x: x[\"rerank\"], reverse=True)[:topk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef7dcfb-573f-4ead-94b4-ab0f60a9e11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: what does recon-all -all do?\n",
      "  • 0.896 | fmriprep.org_en_stable_faq.html.md\n",
      "    (unknown-url) fmriprep.org_en_stable_faq.html.md\n",
      "    the Linux kernel to kill processes as a response to running out of memory. Depending on the process killed by the kernel, *fMRIPrep* may crash with a `BrokenProcessPool` error or hang indefinitely, depending on settings. While we are working on finding a solution that does not run up against this bug, this may take some time. This can be most easily resolved by allocating more memory to the proces...\n",
      "  • 0.814 | fmriprep.org_en_stable_workflows.html.md\n",
      "    (unknown-url) fmriprep.org_en_stable_workflows.html.md\n",
      "    ### Surface preprocessing[](https://fmriprep.org#surface-preprocessing) *fMRIPrep* uses [FreeSurfer](https://surfer.nmr.mgh.harvard.edu/) to reconstruct surfaces from T1w/T2w structural images. If enabled, several steps in the *fMRIPrep* pipeline are added or replaced. All surface preprocessing may be disabled with the `--fs-no-reconall` flag. Note Surface processing will be skipped if the outputs...\n",
      "  • 0.803 | fmriprep.org_en_stable_faq.html.md\n",
      "    (unknown-url) fmriprep.org_en_stable_faq.html.md\n",
      "    Current Stamp: freesurfer-Linux-centos6_x86_64-stable-pub-v6.0.1-f53a55a INFO: SUBJECTS_DIR is /outputs/freesurfer Actual FREESURFER_HOME /opt/freesurfer -rw-rw-r-- 1 11239 users 207798 Apr 1 16:19 /outputs/freesurfer/sub-020/scripts/recon-all.log Linux 62324c0da859 4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux ERROR: it appears that recon-all is alr...\n",
      "\n",
      "Q: how to run fmriprep participant level\n",
      "  • 0.994 | fmriprep.org_en_stable_usage.html.md\n",
      "    (unknown-url) fmriprep.org_en_stable_usage.html.md\n",
      "    ### Positional Arguments[](https://fmriprep.org#fmriprep.cli.parser-_build_parser-positional-arguments)  `bids_dir`The root folder of a BIDS valid dataset (sub-XXXXX folders should be found at the top level in this folder).  `output_dir`The output path for the outcomes of preprocessing and visual reports  `analysis_level`Possible choices: participant  Processing stage to be run, only “participant”...\n",
      "  • 0.990 | fmriprep.org_en_stable_usage.html.md\n",
      "    (unknown-url) fmriprep.org_en_stable_usage.html.md\n",
      "    ## Execution and the BIDS format[](https://fmriprep.org#execution-and-the-bids-format)  The *fMRIPrep* workflow takes as principal input the path of the dataset that is to be processed. The input dataset is required to be in valid BIDS format, and it must include at least one T1w structural image and (unless disabled with a flag) a BOLD series. We highly recommend that you validate your dataset wi...\n",
      "  • 0.970 | fmriprep.org_en_stable_benchmarks.html.md\n",
      "    (unknown-url) fmriprep.org_en_stable_benchmarks.html.md\n",
      "    ### fMRIPrep versions and modes[](https://fmriprep.org#fmriprep-versions-and-modes)  All commands took the form `fmriprep sourcedata/raw . participant $OPTIONS`  . The specific options for each version or mode are presented in the following table.  Version / Mode | Options | |---|---| 23.1.4 | | 23.2.0a2 / fit | | 23.2.0a2 / fit + transform | |\n",
      "\n",
      "Q: what outputs are generated by fmriprep\n",
      "  • 0.997 | fmriprep.org_en_stable_outputs.html.md\n",
      "    (unknown-url) fmriprep.org_en_stable_outputs.html.md\n",
      "    # Outputs of *fMRIPrep*[](https://fmriprep.org#outputs-of-fmriprep)  *fMRIPrep* outputs conform to the BIDS Derivatives specification (see [BIDS Derivatives](https://bids-specification.readthedocs.io/en/stable/05-derivatives/01-introduction.html), along with the upcoming [BEP 011](https://bids-specification.readthedocs.io/en/bep011/05-derivatives/04-structural-derivatives.html) and [BEP 012](https...\n",
      "  • 0.987 | fmriprep.org_en_stable_outputs.html.md\n",
      "    (unknown-url) fmriprep.org_en_stable_outputs.html.md\n",
      "    ## Layout[](https://fmriprep.org#layout)  Assuming fMRIPrep is invoked with:  ``` fmriprep <input_dir>/ <output_dir>/ participant [OPTIONS] ```  The outputs will be a [BIDS Derivatives](https://bids-specification.readthedocs.io/en/stable/05-derivatives/01-introduction.html) dataset of the form:  ``` <output_dir>/ logs/ sub-<label>/ sub-<label>.html dataset_description.json .bidsignore ```  For eac...\n",
      "  • 0.983 | fmriprep.org_en_stable_workflows.html.md\n",
      "    (unknown-url) fmriprep.org_en_stable_workflows.html.md\n",
      "    # Processing pipeline details[](https://fmriprep.org#processing-pipeline-details)  *fMRIPrep* adapts its pipeline depending on what data and metadata are available and are used as the input. For example, slice timing correction will be performed only if the `SliceTiming`  metadata field is found for the input dataset.  A (very) high-level view of the simplest pipeline (for a single-band dataset wi...\n"
     ]
    }
   ],
   "source": [
    "def search(q, k=5, use_hybrid=True, use_rerank=True):\n",
    "    if use_hybrid:\n",
    "        cands = rrf_fuse(q, k=max(k*4, 20))   # get a wider candidate set\n",
    "    else:\n",
    "        # dense only\n",
    "        D, I = index.search(model.encode([q], convert_to_numpy=True, normalize_embeddings=True), max(k*4, 20))\n",
    "        cands = [{\"text\": texts[i], \"source_url\": meta[i][\"source_url\"], \"source_file\": meta[i][\"source_file\"], \"score\": float(s)}\n",
    "                 for i, s in zip(I[0], D[0])]\n",
    "    if use_rerank:\n",
    "        cands = rerank(q, cands, topk=k)\n",
    "    else:\n",
    "        cands = cands[:k]\n",
    "    # pretty view\n",
    "    results = []\n",
    "    for c in cands:\n",
    "        results.append({\n",
    "            \"source_file\": c[\"source_file\"],\n",
    "            \"source_url\": c[\"source_url\"],\n",
    "            \"score\": float(c.get(\"rerank\", c.get(\"score\", 0.0))),\n",
    "            \"text\": c[\"text\"][:400].replace(\"\\n\", \" \") + (\"...\" if len(c[\"text\"])>400 else \"\")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# quick tests\n",
    "for q in [\n",
    "    \"what does recon-all -all do?\",\n",
    "    \"how to run fmriprep participant level\",\n",
    "    \"what outputs are generated by fmriprep\"\n",
    "]:\n",
    "    print(\"\\nQ:\", q)\n",
    "    for h in search(q, k=3, use_hybrid=True, use_rerank=True):\n",
    "        print(f\"  • {h['score']:.3f} | {h['source_file']}\")\n",
    "        print(f\"    {h['source_url']}\")\n",
    "        print(\"   \", h[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa688db-fde5-49ef-937c-2f9477bf83c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
