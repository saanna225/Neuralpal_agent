{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip -q install requests beautifulsoup4 lxml markdownify fastembed onnxruntime chromadb rank-bm25 pymupdf\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "SITES_HTML = [\n",
    "    (\"BIDS\",     \"https://bids-specification.readthedocs.io/en/stable/\", \"/en/stable/\"),\n",
    "    (\"fMRIPrep\", \"https://fmriprep.readthedocs.io/en/stable/\",           \"/en/stable/\"),\n",
    "]\n",
    "SITES_PDF = [\n",
    "    (\"MRtrix\",   \"https://media.readthedocs.org/pdf/mrtrix/latest/mrtrix.pdf\"),\n",
    "    (\"SPM12\",    \"https://www.fil.ion.ucl.ac.uk/spm/doc/spm12_manual.pdf\"),\n",
    "]\n",
    "\n",
    "ALLOW_SUBSTR = {  \n",
    "    \"BIDS\":     [\"specification\", \"glossary\", \"derivatives\", \"intro\"],\n",
    "    \"fMRIPrep\": [\"usage\", \"installation\", \"faq\", \"outputs\", \"reports\"],\n",
    "}\n",
    "COL_NAME  = \"neuro_docs\"\n",
    "MAX_PAGES_PER_SITE = 40  #could be increased\n",
    "BATCH = 128  \n",
    "\n",
    "# ---------------- IMPORTS ----------------\n",
    "import os, re, time, requests, fitz, numpy as np\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "from fastembed import TextEmbedding\n",
    "from rank_bm25 import BM25Okapi\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ---------------- HELPERS ----------------\n",
    "slug = lambda s: re.sub(r\"[^a-z0-9._-]+\",\"-\", (s or \"\").strip().lower())\n",
    "def save_text(p: Path, text: str):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True); p.write_text(text, encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def html_to_md(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    main = soup.select_one(\"main, .md-content, div.body, div.document\") or soup\n",
    "    text = md(str(main), heading_style=\"ATX\")\n",
    "    text = re.sub(r\"(?s)Next\\s+Previous.*$\", \"\", text)             \n",
    "    text = re.sub(r\"Â©.*?Read the Docs.*$\", \"\", text)\n",
    "    return text\n",
    "# crawling websites :\n",
    "def crawl_html(tool, base, prefix, outdir: Path, max_pages=120):\n",
    "    host = urlsplit(base).netloc\n",
    "    seen, stack, saved = set(), [base], 0\n",
    "    skip_ext = re.compile(r\"\\.(png|jpg|gif|svg|pdf|zip|tar\\.gz|ico)$\", re.I)\n",
    "    allow = ALLOW_SUBSTR.get(tool)\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    def ok(url):\n",
    "        p = urlsplit(url)\n",
    "        if p.netloc != host or skip_ext.search(p.path): return False\n",
    "        if url == base: return True                      \n",
    "        if not p.path.startswith(prefix): return False\n",
    "        return True if not allow else any(s in p.path for s in allow)\n",
    "    while stack and saved < max_pages:\n",
    "        url = stack.pop()\n",
    "        if url in seen or not ok(url): continue\n",
    "        seen.add(url)\n",
    "        try:\n",
    "            html = requests.get(url, timeout=30, headers=headers).text\n",
    "        except Exception:\n",
    "            continue\n",
    "        rel = (urlsplit(url).path or \"/\").strip(\"/\")\n",
    "        save_text(outdir / f\"{slug(rel or 'index')}.md\", html_to_md(html))\n",
    "        saved += 1\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        for a in soup.select(\"a[href]\"):\n",
    "            stack.append(urljoin(url, a[\"href\"]))\n",
    "    print(f\"[{tool}] HTML pages saved: {saved} -> {outdir}\") #saving them\n",
    "\n",
    "#  PDF downloader \n",
    "def fetch_pdf(url: str, out_dir: Path) -> Path:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    name = slug(Path(urlsplit(url).path).name or \"doc.pdf\")\n",
    "    if not name.endswith(\".pdf\"): name += \".pdf\"\n",
    "    out_path = out_dir / name\n",
    "    sess = requests.Session()\n",
    "    sess.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    sess.mount(\"https://\", HTTPAdapter(max_retries=Retry(total=5, backoff_factor=0.4,\n",
    "                         status_forcelist=[429,500,502,503,504], allowed_methods=[\"GET\",\"HEAD\"])))\n",
    "    r = sess.get(url, timeout=60, stream=True, allow_redirects=True); r.raise_for_status()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        for ch in r.iter_content(chunk_size=1<<15):\n",
    "            if ch: f.write(ch)\n",
    "    return out_path\n",
    "\n",
    "def pdf_paragraphs(pdf_path: Path): #prep for chunking and parsing\n",
    "    def clean(s: str) -> str:\n",
    "        s = re.sub(r'-\\n(?=\\w)', '', s); s = re.sub(r'\\n(?=[a-z])', ' ', s)\n",
    "        s = re.sub(r'\\s*\\n\\s*', ' ', s); return re.sub(r'\\s{2,}', ' ', s).strip()\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for pno, page in enumerate(doc, 1):\n",
    "        blocks = sorted(page.get_text(\"blocks\"), key=lambda b:(b[1], b[0]))\n",
    "        paras=[]\n",
    "        for b in blocks:\n",
    "            txt = b[4] if len(b)>4 and isinstance(b[4], str) else \"\"\n",
    "            if not txt.strip(): continue\n",
    "            t = clean(txt)\n",
    "            for p in re.split(r'\\n{2,}', t):\n",
    "                if len(p.split())>6: paras.append(p.strip())\n",
    "        for j, para in enumerate(paras, 1):\n",
    "            yield pno, j, para\n",
    "\n",
    "def chunk_md(text: str, size=480, overlap=60): #chunking\n",
    "    words = re.split(r\"\\s+\", text.strip()); \n",
    "    if not words: return\n",
    "    step = max(1, size-overlap)\n",
    "    for i in range(0, len(words), step):\n",
    "        piece = \" \".join(words[i:i+size]).strip()\n",
    "        if len(piece.split()) > 30: yield piece\n",
    "\n",
    "def reflow(t: str) -> str:\n",
    "    t = re.sub(r'-\\n(?=\\w)', '', t); t = re.sub(r'\\s*\\n\\s*', ' ', t)\n",
    "    return re.sub(r'\\s{2,}', ' ', t).strip()\n",
    "\n",
    "ROOT = Path(\"corpus_multi\")\n",
    "all_docs, all_ids, all_metas = [], [], []\n",
    "\n",
    "# HTML (BIDS + fMRIPrep)\n",
    "for tool, base, pref in SITES_HTML:\n",
    "    out = ROOT / \"html\" / slug(tool)\n",
    "    crawl_html(tool, base, pref or urlsplit(base).path, out, max_pages=MAX_PAGES_PER_SITE)\n",
    "    for mdfile in sorted(out.rglob(\"*.md\")):\n",
    "        txt = mdfile.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        for k, chunk in enumerate(chunk_md(txt), 1):\n",
    "            all_docs.append(chunk)\n",
    "            all_ids.append(f\"{tool}__{mdfile.stem}__chunk{k:03}\")\n",
    "            all_metas.append({\"tool\": tool, \"source\": str(mdfile)})\n",
    "\n",
    "# PDFs (MRtrix + SPM12)\n",
    "PDF_DIR = ROOT / \"pdf\"; PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for tool, url in SITES_PDF:\n",
    "    try:\n",
    "        pdf_path = fetch_pdf(url, PDF_DIR / slug(tool))\n",
    "        print(f\"[{tool}] saved PDF:\", pdf_path.name)\n",
    "    except Exception as e:\n",
    "        print(f\"[{tool}] SKIP {url} ->\", e); continue\n",
    "    for pno, j, para in pdf_paragraphs(pdf_path):\n",
    "        all_docs.append(para)\n",
    "        all_ids.append(f\"{tool}__{pdf_path.stem}__p{pno:03}__para{j:02}\")\n",
    "        all_metas.append({\"tool\": tool, \"source\": str(pdf_path), \"page\": pno, \"para\": j})\n",
    "\n",
    "print(\"TOTAL chunks:\", len(all_docs))\n",
    "\n",
    "# Embedding\n",
    "client = chromadb.PersistentClient(path=\"vector_db\")\n",
    "col    = client.get_or_create_collection(COL_NAME)\n",
    "existing = set()\n",
    "try:\n",
    "    got = col.get(include=[], limit=100000)\n",
    "    for _id in got.get(\"ids\", []): existing.add(_id)\n",
    "except Exception: pass\n",
    "\n",
    "embedder = TextEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "t0 = time.time()\n",
    "for i in range(0, len(all_docs), BATCH):\n",
    "    docs_b = all_docs[i:i+BATCH]\n",
    "    ids_b  = all_ids[i:i+BATCH]\n",
    "    mets_b = all_metas[i:i+BATCH]\n",
    "    keep = [j for j,_id in enumerate(ids_b) if _id not in existing]\n",
    "    if not keep:\n",
    "        continue\n",
    "\n",
    "    docs_b = [str(x) for x in docs_b]                     # ensure strings\n",
    "    ids_b  = [str(x) for x in ids_b]                      # <- important: force str\n",
    "    mets_b = [dict(m) for m in mets_b]  \n",
    "\n",
    "    embs_b = [list(map(float, v)) for v in embedder.embed(docs_b)]\n",
    "\n",
    "    assert len(ids_b) == len(docs_b) == len(mets_b) == len(embs_b) > 0\n",
    "\n",
    "    col.upsert(\n",
    "        ids=ids_b,\n",
    "        embeddings=embs_b,\n",
    "        metadatas=mets_b,\n",
    "        documents=docs_b\n",
    "    )\n",
    "    existing.update(ids_b)\n",
    "    if (i//BATCH) % 2 == 0 or i+BATCH >= len(all_docs):\n",
    "        print(f\"Indexed {min(i+BATCH,len(all_docs))}/{len(all_docs)}\")\n",
    "\n",
    "print(\"Embedding done.\")\n",
    "\n",
    "#retrival action\n",
    "from collections import defaultdict\n",
    "\n",
    "file_to_idxs = defaultdict(list)\n",
    "for i,m in enumerate(all_metas): file_to_idxs[m[\"source\"]].append(i)\n",
    "file_list  = sorted(file_to_idxs.keys())\n",
    "file_texts = [\" \".join(all_docs[i] for i in file_to_idxs[f]) for f in file_list]\n",
    "bm25_doc   = BM25Okapi([t.split() for t in file_texts])\n",
    "id_to_idx  = {i:k for k,i in enumerate(all_ids)}\n",
    "\n",
    "def _z(a):\n",
    "    a = np.asarray(a, float); return (a - a.mean()) / (a.std() + 1e-6)\n",
    "# checking first always preferred doc\n",
    "def best_idx_docfirst(q, k_files=8, k_vec=80, k_bm=80, w_vec=0.7, w_bm=0.3):\n",
    "    qtok = q.split()\n",
    "#checking file scores\n",
    "    scores_doc = bm25_doc.get_scores(qtok)                    \n",
    "    top_idx = np.argsort(scores_doc)[::-1][:max(1, min(k_files, len(scores_doc)))]\n",
    "    cand_files = [file_list[i] for i in top_idx] if len(file_list) else []\n",
    "    if not cand_files:                                      \n",
    "        cand_files = file_list[:min(8, len(file_list))]\n",
    "    cand_idxs = [i for f in cand_files for i in file_to_idxs.get(f, [])]\n",
    "    if not cand_idxs: #(getting error here)\n",
    "        cand_files = file_list[:min(16, len(file_list))]\n",
    "        cand_idxs  = [i for f in cand_files for i in file_to_idxs.get(f, [])]\n",
    "    q_emb = next(embedder.embed([q])) #vectorsearch\n",
    "    if cand_files:\n",
    "        v = col.query(query_embeddings=[q_emb.tolist()],\n",
    "                      n_results=min(k_vec, len(all_docs)),\n",
    "                      where={\"source\": {\"$in\": cand_files}},\n",
    "                      include=[\"distances\"])\n",
    "    else:\n",
    "        # global fallback\n",
    "        v = col.query(query_embeddings=[q_emb.tolist()],\n",
    "                      n_results=min(k_vec, len(all_docs)),\n",
    "                      include=[\"distances\"])\n",
    "\n",
    "    vec_idxs = [id_to_idx[i] for i in v.get(\"ids\", [[]])[0] if i in id_to_idx]\n",
    "    vec_sims = {j: -d for j, d in zip(vec_idxs, v.get(\"distances\", [[]])[0])}\n",
    "    pool_for_bm = cand_idxs if cand_idxs else (vec_idxs if vec_idxs else [])\n",
    "    if pool_for_bm:\n",
    "        bm_local = BM25Okapi([all_docs[i].split() for i in pool_for_bm])\n",
    "        local_scores = bm_local.get_scores(qtok)\n",
    "        bm_scores = {pool_for_bm[j]: float(local_scores[j]) for j in range(len(pool_for_bm))}\n",
    "    else:\n",
    "        bm_scores = {}\n",
    "\n",
    "    pool = set(vec_sims.keys()) | set(pool_for_bm) #chunk fusion\n",
    "    if not pool:\n",
    "        v2 = col.query(query_embeddings=[q_emb.tolist()], n_results=1, include=[\"ids\"])\n",
    "        return id_to_idx[v2[\"ids\"][0][0]]\n",
    "\n",
    "    v_list = [vec_sims.get(i, 0.0) for i in pool]\n",
    "    b_list = [bm_scores.get(i, 0.0) for i in pool]\n",
    "    v_z, b_z = _z(v_list), _z(b_list)\n",
    "    fused = {i: (w_vec*v + w_bm*b) for i, (v, b) in zip(pool, zip(v_z, b_z))}\n",
    "    file_scores = {}   #chunk the good scorer file\n",
    "    for i, s in fused.items():\n",
    "        f = all_metas[i][\"source\"]\n",
    "        file_scores[f] = max(file_scores.get(f, -1e9), s)\n",
    "    top_file = max(file_scores, key=file_scores.get)\n",
    "    best_chunk = max((i for i in pool if all_metas[i][\"source\"] == top_file), key=lambda i: fused[i])\n",
    "    return best_chunk\n",
    "# finding answers to the prompts\n",
    "def answer(q, neighbors=1, max_chars=900):\n",
    "    i = best_idx_docfirst(q)\n",
    "    m = all_metas[i]; src = m[\"source\"]; page = m.get(\"page\")\n",
    "    same = [k for k,x in enumerate(all_metas) if x[\"source\"]==src and x.get(\"page\")==page]\n",
    "    if not same: same = [k for k,x in enumerate(all_metas) if x[\"source\"]==src]\n",
    "    same.sort(); pos = same.index(i)\n",
    "    win = same[max(0,pos-neighbors):pos+neighbors+1]\n",
    "    txt = re.sub(r'\\s{2,}',' ', re.sub(r'\\s*\\n\\s*',' ', \" \".join(all_docs[k] for k in win))).strip()\n",
    "    if len(txt) > max_chars: txt = txt[:max_chars].rsplit(\". \",1)[0] + \".\"\n",
    "    cite = f\"{Path(src).name}\" + (f\" (p{page})\" if page else \"\")\n",
    "    return txt, cite\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
